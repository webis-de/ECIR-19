# Web scale text reuse analysis!

This work aims to understand the text reuse phenomena in Wikipedia and to quantify the amount of Wikipedia content reused on the web. In particular our research questions are; (1) What kinds of text reuse cases in Wikipedia? and (2) How much of the content in the web is just a reuse of Wikipedia content? following that is the question of how much Ad revenue is generated by this reused content?
  	
 To answer these questions, the first contribution of this thesis is building a framework for extracting text reuse cases from big datasets. The framework is a form of a pipeline of tasks that use heuristic based algorithms which run in a distributed manner on a cluster of machines. Using this framework we extracted 100 million text reuse cases from Wikipedia articles and 1.6 million text reuse cases between Wikipedia and a sample of the web. Besides that, this framework could be further used in future work to extract text reuse cases from other big datasets.
  	
 The second contribution of this work is analyzing the extracted text reuse cases from Wikipedia. We describe two situations in which a piece of text is reused and provide simple heuristics to automatically classify text reuse cases.
  	
 Third, we present and describe the text reuse cases that was extracted in between Wikipedia and the web. We quantify the amount of reused content in a sample of the web and present a basic approach for estimating the Ad revenue generated by reusing the free content of Wikipedia.

# Text Reuse Pipeline

To build the Text reuse pipeline python module, run the following command:

    make build
 This will generate a `dist` folder. All the tasks could be run under the `dist` folder.

## Wikipedia dump extraction

Using the open source [tool](https://github.com/attardi/wikiextractor), we run the following command to extract the text from the Wiki text:

    python wiki_extractor.py -b 30G -s -ns 0 --filter_disambig_pages -o wiki_no_lists  enwiki-20160501-pages-articles.xml.bz2 &

The output should be copied to hdfs files using the following command: `hdfs dfs -put ./path_of_the_extracted_dump ./text-reuse/wiki_00`

## Wikipedia Text preprocessing

To perform text preprocessing on Wikipedia, Run the following two commands inside webis docker [image
](http://gitlab.webis.de). Assuming that the extracted Wikipedia dump is located under the following hdfs path: `text-reuse/wiki_00`
	

	## To perform paragraph re-balancing and cleaning up text
    PYSPARK_DRIVER_PYTHON=python3 spark-submit --master yarn --deploy-mode cluster --num-executors 100 --executor-cores 10 --executor-memory 25g --driver-memory 25g --conf spark.driver.maxResultSize=15g --conf spark.yarn.executor.memoryOverhead=25000 --conf spark.yarn.driver.memoryOverhead=25000 --packages com.databricks:spark-xml_2.11:0.4.1,com.databricks:spark-csv_2.10:1.5.0 --py-files ./text_reuse_pipeline.zip main.py --job wiki_preprocess
    ## The output is written to the following hdfs path: `text-reuse/pipeline/wiki_preprocessed

    ## To extract TFIDF vectors for each article
    PYSPARK_DRIVER_PYTHON=python3 spark-submit --master yarn --deploy-mode cluster --num-executors 100 --executor-cores 10 --executor-memory 25g --driver-memory 25g --conf spark.driver.maxResultSize=15g --conf spark.yarn.executor.memoryOverhead=25000 --conf spark.yarn.driver.memoryOverhead=25000 --packages com.databricks:spark-xml_2.11:0.4.1,com.databricks:spark-csv_2.10:1.5.0 --py-files ./text_reuse_pipeline.zip main.py --job wiki_represent --job_args tfidf
    ## The output is written to the following hdfs path: text-reuse/pipeline/wiki_rep_tfidf



## Wikipedia candidate elimination

To extract candidate articles from Wikipedia to be examined in the last subtask, we run the following command:

    PYSPARK_DRIVER_PYTHON=python3 spark-submit --master yarn --deploy-mode cluster --num-executors 200 --executor-cores 10 --executor-memory 25g --driver-memory 25g --conf spark.driver.maxResultSize=15g --conf spark.yarn.executor.memoryOverhead=25000 --conf spark.yarn.driver.memoryOverhead=25000 --packages com.databricks:spark-xml_2.11:0.4.1,com.databricks:spark-csv_2.10:1.5.0 --py-files ./text_reuse_pipeline.zip main.py --job wiki_candidate_elemination --job_args hdfs://betaweb020:8020/user/sile2804/cython_utils.so 0-10 0.025

This candidate elimination is divided into 100 batches. In the command 0-10 means perform candidate elimination on the batches from 0 to 10. The last argument is 0.025 is the threshold that we consider as the minimum similarity between two documents to be considered similar.

## Wikipedia text alignments
To run the detailed alignments on the candidate elimination output. Run the following command:

    PYSPARK_DRIVER_PYTHON=python3 spark-submit --master yarn --deploy-mode cluster --num-executors 200 --executor-cores 2 --executor-memory 45g --driver-memory 45g --conf spark.driver.maxResultSize=15g --packages com.databricks:spark-xml_2.11:0.4.1,com.databricks:spark-csv_2.10:1.5.0 --jars /home/sile2804/picapica.jar --py-files ./text_reuse_pipeline.zip main.py --job wiki_text_alignment --job_args text-reuse/pipeline/candidates/[0-2] text-reuse/pipeline/alignments/output_name threshold k &

Arguments for this command:
-  text-reuse/pipeline/candidates/[0-2] : hdfs input path (here it means run on parts 0-2 of the whole dataset) 
 - text-reuse/pipeline/alignments/output_name: hdfs output path
 - threshold is the minimum similarity threshold
 - k number of misses threshold

# Text Reuse Analysis

## Python notebooks:

You can save any file of the workspace to **Google Drive**, **Dropbox** or **GitHub** by opening the **Synchronize** sub-menu and clicking **Save on**. Even if a file in the workspace is already synced, you can save it to another location. StackEdit can sync one file with multiple locations and accounts.

## Web tool for text reuse exploration
